{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-0595f8949b9a>:113: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Could not find old network weights\n",
      "TIMESTEP 1 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 3 / REWARD 20 / Q_MAX 1.152538e-02\n",
      "TIMESTEP 2 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 3 / REWARD 20 / Q_MAX 1.079576e-02\n",
      "TIMESTEP 3 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 0 / REWARD 20 / Q_MAX 1.086836e-02\n",
      "TIMESTEP 4 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 20 / Q_MAX 1.049772e-02\n",
      "TIMESTEP 5 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 3 / REWARD 20 / Q_MAX 1.293637e-02\n",
      "TIMESTEP 6 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 0 / REWARD 20 / Q_MAX 1.092410e-02\n",
      "TIMESTEP 7 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 0 / REWARD 20 / Q_MAX 1.028563e-02\n",
      "TIMESTEP 8 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 4 / REWARD 16 / Q_MAX 1.021393e-02\n",
      "TIMESTEP 9 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 0 / REWARD 18 / Q_MAX 8.870278e-03\n",
      "TIMESTEP 10 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 18 / Q_MAX 9.528076e-03\n",
      "TIMESTEP 11 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 3 / REWARD 18 / Q_MAX 1.172986e-02\n",
      "TIMESTEP 12 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 0 / REWARD 18 / Q_MAX 1.322500e-02\n",
      "TIMESTEP 13 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 0 / REWARD 18 / Q_MAX 1.409387e-02\n",
      "TIMESTEP 14 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 5 / REWARD 18 / Q_MAX 1.259522e-02\n",
      "TIMESTEP 15 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 4 / REWARD 16 / Q_MAX 1.482843e-02\n",
      "TIMESTEP 16 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 0 / REWARD 17 / Q_MAX 1.043671e-02\n",
      "TIMESTEP 17 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 5 / REWARD 17 / Q_MAX 1.104697e-02\n",
      "TIMESTEP 18 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 1 / REWARD 17 / Q_MAX 1.127547e-02\n",
      "TIMESTEP 19 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 1 / REWARD 17 / Q_MAX 1.387280e-02\n",
      "TIMESTEP 20 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 3 / REWARD 17 / Q_MAX 1.417436e-02\n",
      "TIMESTEP 21 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 3 / REWARD 17 / Q_MAX 1.381641e-02\n",
      "TIMESTEP 22 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 17 / Q_MAX 1.530934e-02\n",
      "TIMESTEP 23 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 0 / REWARD 17 / Q_MAX 1.309423e-02\n",
      "TIMESTEP 24 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 17 / Q_MAX 1.447225e-02\n",
      "TIMESTEP 25 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 0 / REWARD 17 / Q_MAX 1.450128e-02\n",
      "TIMESTEP 26 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 17 / Q_MAX 1.520437e-02\n",
      "TIMESTEP 27 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 4 / REWARD 13 / Q_MAX 1.506216e-02\n",
      "TIMESTEP 28 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 15 / Q_MAX 1.476684e-02\n",
      "TIMESTEP 29 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 1 / REWARD 15 / Q_MAX 1.541381e-02\n",
      "TIMESTEP 30 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 0 / REWARD 15 / Q_MAX 1.712679e-02\n",
      "TIMESTEP 31 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 3 / REWARD 15 / Q_MAX 1.421133e-02\n",
      "TIMESTEP 32 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 15 / Q_MAX 1.618891e-02\n",
      "TIMESTEP 33 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 15 / Q_MAX 1.703320e-02\n",
      "TIMESTEP 34 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 1 / REWARD 15 / Q_MAX 1.714206e-02\n",
      "TIMESTEP 35 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 0 / REWARD 15 / Q_MAX 1.617659e-02\n",
      "TIMESTEP 36 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 4 / REWARD 15 / Q_MAX 1.190780e-02\n",
      "TIMESTEP 37 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 0 / REWARD 15 / Q_MAX 1.132780e-02\n",
      "TIMESTEP 38 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 1 / REWARD 15 / Q_MAX 1.373519e-02\n",
      "TIMESTEP 39 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 15 / Q_MAX 1.409182e-02\n",
      "TIMESTEP 40 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 1 / REWARD 15 / Q_MAX 1.353651e-02\n",
      "TIMESTEP 41 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 0 / REWARD 15 / Q_MAX 1.527862e-02\n",
      "TIMESTEP 42 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 5 / REWARD 15 / Q_MAX 1.588407e-02\n",
      "TIMESTEP 43 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 1 / REWARD 15 / Q_MAX 1.345462e-02\n",
      "TIMESTEP 44 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 1 / REWARD 15 / Q_MAX 1.369509e-02\n",
      "TIMESTEP 45 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 4 / REWARD 15 / Q_MAX 1.573464e-02\n",
      "TIMESTEP 46 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 3 / REWARD 15 / Q_MAX 1.487639e-02\n",
      "TIMESTEP 47 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 4 / REWARD 13 / Q_MAX 1.456201e-02\n",
      "TIMESTEP 48 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 14 / Q_MAX 1.392120e-02\n",
      "TIMESTEP 49 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 14 / Q_MAX 1.426827e-02\n",
      "TIMESTEP 50 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 14 / Q_MAX 1.453564e-02\n",
      "TIMESTEP 51 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 4 / REWARD 10 / Q_MAX 1.688022e-02\n",
      "TIMESTEP 52 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 4 / REWARD 10 / Q_MAX 1.870567e-02\n",
      "TIMESTEP 53 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 5 / REWARD 11 / Q_MAX 2.156568e-02\n",
      "TIMESTEP 54 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 0 / REWARD 11 / Q_MAX 2.280540e-02\n",
      "TIMESTEP 55 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 0 / REWARD 11 / Q_MAX 1.986995e-02\n",
      "TIMESTEP 56 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 1 / REWARD 11 / Q_MAX 1.920421e-02\n",
      "TIMESTEP 57 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 5 / REWARD 11 / Q_MAX 1.996743e-02\n",
      "TIMESTEP 58 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 5 / REWARD 11 / Q_MAX 1.604171e-02\n",
      "TIMESTEP 59 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 0 / REWARD 11 / Q_MAX 2.161453e-02\n",
      "TIMESTEP 60 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 0 / REWARD 11 / Q_MAX 1.837888e-02\n",
      "TIMESTEP 61 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 4 / REWARD 7 / Q_MAX 1.998557e-02\n",
      "TIMESTEP 62 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 0 / REWARD 9 / Q_MAX 1.913034e-02\n",
      "TIMESTEP 63 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 0 / REWARD 9 / Q_MAX 1.793861e-02\n",
      "TIMESTEP 64 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 5 / REWARD 9 / Q_MAX 1.628728e-02\n",
      "TIMESTEP 65 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 0 / REWARD 9 / Q_MAX 1.501529e-02\n",
      "TIMESTEP 66 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 0 / REWARD 9 / Q_MAX 1.685060e-02\n",
      "TIMESTEP 67 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 4 / REWARD 9 / Q_MAX 1.912310e-02\n",
      "TIMESTEP 68 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 5 / REWARD 9 / Q_MAX 2.108883e-02\n",
      "TIMESTEP 69 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 1 / REWARD 9 / Q_MAX 1.995549e-02\n",
      "TIMESTEP 70 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 9 / Q_MAX 1.885589e-02\n",
      "TIMESTEP 71 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 9 / Q_MAX 1.674437e-02\n",
      "TIMESTEP 72 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 1 / REWARD 9 / Q_MAX 2.109461e-02\n",
      "TIMESTEP 73 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 0 / REWARD 9 / Q_MAX 1.616074e-02\n",
      "TIMESTEP 74 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 5 / REWARD 9 / Q_MAX 2.089058e-02\n",
      "TIMESTEP 75 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 1 / REWARD 9 / Q_MAX 1.742768e-02\n",
      "TIMESTEP 76 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 0 / REWARD 9 / Q_MAX 2.084673e-02\n",
      "TIMESTEP 77 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 0 / REWARD 9 / Q_MAX 1.979490e-02\n",
      "TIMESTEP 78 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 9 / Q_MAX 1.924614e-02\n",
      "TIMESTEP 79 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 5 / REWARD 9 / Q_MAX 1.869761e-02\n",
      "TIMESTEP 80 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 5 / REWARD 9 / Q_MAX 1.883256e-02\n",
      "TIMESTEP 81 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 1 / REWARD 9 / Q_MAX 1.927222e-02\n",
      "TIMESTEP 82 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 5 / REWARD 9 / Q_MAX 1.836950e-02\n",
      "TIMESTEP 83 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 1 / REWARD 9 / Q_MAX 1.757398e-02\n",
      "TIMESTEP 84 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 4 / REWARD 7 / Q_MAX 1.655387e-02\n",
      "TIMESTEP 85 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 5 / REWARD 8 / Q_MAX 1.563483e-02\n",
      "TIMESTEP 86 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 0 / REWARD 8 / Q_MAX 1.728211e-02\n",
      "TIMESTEP 87 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 3 / REWARD 8 / Q_MAX 1.690091e-02\n",
      "TIMESTEP 88 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 3 / REWARD 8 / Q_MAX 2.039701e-02\n",
      "TIMESTEP 89 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 8 / Q_MAX 2.017441e-02\n",
      "TIMESTEP 90 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 0 / REWARD 8 / Q_MAX 1.889019e-02\n",
      "TIMESTEP 91 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 5 / REWARD 8 / Q_MAX 1.872108e-02\n",
      "TIMESTEP 92 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 4 / REWARD 8 / Q_MAX 1.801835e-02\n",
      "TIMESTEP 93 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 0 / REWARD 8 / Q_MAX 1.715915e-02\n",
      "TIMESTEP 94 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 8 / Q_MAX 1.784418e-02\n",
      "TIMESTEP 95 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 8 / Q_MAX 1.912769e-02\n",
      "TIMESTEP 96 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 5 / REWARD 8 / Q_MAX 2.104046e-02\n",
      "TIMESTEP 97 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 0 / REWARD 8 / Q_MAX 2.013051e-02\n",
      "TIMESTEP 98 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 1 / REWARD 8 / Q_MAX 1.982537e-02\n",
      "TIMESTEP 99 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 1 / REWARD 4 / Q_MAX 2.204045e-02\n",
      "TIMESTEP 100 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 4 / REWARD 6 / Q_MAX 2.165954e-02\n",
      "TIMESTEP 101 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 4 / REWARD 4 / Q_MAX 2.193281e-02\n",
      "TIMESTEP 102 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 5 / REWARD 5 / Q_MAX 2.030633e-02\n",
      "TIMESTEP 103 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 1 / Q_MAX 2.122525e-02\n",
      "TIMESTEP 104 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 3 / REWARD 3 / Q_MAX 2.219606e-02\n",
      "TIMESTEP 105 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 0 / REWARD 3 / Q_MAX 2.603777e-02\n",
      "TIMESTEP 106 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 3 / Q_MAX 2.407648e-02\n",
      "TIMESTEP 107 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 0 / Q_MAX 2.367091e-02\n",
      "TIMESTEP 108 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 3 / REWARD 20 / Q_MAX 2.541889e-02\n",
      "TIMESTEP 109 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 20 / Q_MAX 1.794492e-02\n",
      "TIMESTEP 110 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 1 / REWARD 20 / Q_MAX 1.343251e-02\n",
      "TIMESTEP 111 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 0 / REWARD 20 / Q_MAX 1.331035e-02\n",
      "TIMESTEP 112 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 3 / REWARD 20 / Q_MAX 1.087283e-02\n",
      "TIMESTEP 113 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 0 / REWARD 20 / Q_MAX 1.119511e-02\n",
      "TIMESTEP 114 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 20 / Q_MAX 9.561517e-03\n",
      "TIMESTEP 115 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 0 / REWARD 20 / Q_MAX 1.025049e-02\n",
      "TIMESTEP 116 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 20 / Q_MAX 1.012690e-02\n",
      "TIMESTEP 117 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 4 / REWARD 18 / Q_MAX 1.164679e-02\n",
      "TIMESTEP 118 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 19 / Q_MAX 1.133379e-02\n",
      "TIMESTEP 119 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 4 / REWARD 13 / Q_MAX 1.292507e-02\n",
      "TIMESTEP 120 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 1 / REWARD 16 / Q_MAX 1.248973e-02\n",
      "TIMESTEP 121 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 16 / Q_MAX 1.239800e-02\n",
      "TIMESTEP 122 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 4 / REWARD 16 / Q_MAX 1.020804e-02\n",
      "TIMESTEP 123 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 1 / REWARD 16 / Q_MAX 1.079269e-02\n",
      "TIMESTEP 124 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 1 / REWARD 16 / Q_MAX 1.358686e-02\n",
      "TIMESTEP 125 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 16 / Q_MAX 1.248352e-02\n",
      "TIMESTEP 126 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 16 / Q_MAX 1.555149e-02\n",
      "TIMESTEP 127 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 3 / REWARD 16 / Q_MAX 1.294579e-02\n",
      "TIMESTEP 128 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 5 / REWARD 16 / Q_MAX 1.209271e-02\n",
      "TIMESTEP 129 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 1 / REWARD 16 / Q_MAX 1.244691e-02\n",
      "TIMESTEP 130 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 1 / REWARD 16 / Q_MAX 9.029493e-03\n",
      "TIMESTEP 131 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 16 / Q_MAX 1.399540e-02\n",
      "TIMESTEP 132 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 16 / Q_MAX 1.031772e-02\n",
      "TIMESTEP 133 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 5 / REWARD 16 / Q_MAX 1.074200e-02\n",
      "TIMESTEP 134 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 3 / REWARD 16 / Q_MAX 1.137101e-02\n",
      "TIMESTEP 135 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 5 / REWARD 16 / Q_MAX 1.293402e-02\n",
      "TIMESTEP 136 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 3 / REWARD 16 / Q_MAX 1.244267e-02\n",
      "TIMESTEP 137 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 12 / Q_MAX 1.198406e-02\n",
      "TIMESTEP 138 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 3 / REWARD 14 / Q_MAX 1.386837e-02\n",
      "TIMESTEP 139 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 5 / REWARD 14 / Q_MAX 1.532357e-02\n",
      "TIMESTEP 140 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 0 / REWARD 14 / Q_MAX 1.476338e-02\n",
      "TIMESTEP 141 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 14 / Q_MAX 1.489080e-02\n",
      "TIMESTEP 142 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 3 / REWARD 14 / Q_MAX 1.916266e-02\n",
      "TIMESTEP 143 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 4 / REWARD 14 / Q_MAX 1.569050e-02\n",
      "TIMESTEP 144 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 14 / Q_MAX 1.778726e-02\n",
      "TIMESTEP 145 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 4 / REWARD 10 / Q_MAX 1.567795e-02\n",
      "TIMESTEP 146 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 12 / Q_MAX 1.627198e-02\n",
      "TIMESTEP 147 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 5 / REWARD 12 / Q_MAX 1.925427e-02\n",
      "TIMESTEP 148 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 3 / REWARD 12 / Q_MAX 2.028459e-02\n",
      "TIMESTEP 149 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 1 / REWARD 12 / Q_MAX 2.008462e-02\n",
      "TIMESTEP 150 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 3 / REWARD 12 / Q_MAX 2.139413e-02\n",
      "TIMESTEP 151 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 0 / REWARD 12 / Q_MAX 2.065143e-02\n",
      "TIMESTEP 152 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 5 / REWARD 12 / Q_MAX 2.414632e-02\n",
      "TIMESTEP 153 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 12 / Q_MAX 1.867522e-02\n",
      "TIMESTEP 154 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 5 / REWARD 12 / Q_MAX 1.824167e-02\n",
      "TIMESTEP 155 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 10 / Q_MAX 1.667325e-02\n",
      "TIMESTEP 156 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 0 / REWARD 11 / Q_MAX 1.817234e-02\n",
      "TIMESTEP 157 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 11 / Q_MAX 1.818395e-02\n",
      "TIMESTEP 158 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 3 / REWARD 11 / Q_MAX 1.926786e-02\n",
      "TIMESTEP 159 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 1 / REWARD 11 / Q_MAX 2.169815e-02\n",
      "TIMESTEP 160 / STATE observe / LINES 0 / EPSILON 1.0 / ACTION 2 / REWARD 11 / Q_MAX 2.206319e-02\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import sys\n",
    "sys.path.append(\"Wrapped Game Code/\")\n",
    "import pong_fun # whichever is imported \"as game\" will be used\n",
    "import dummy_game\n",
    "import tetris_fun as game\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "GAME = 'tetris' # the name of the game being played for log files\n",
    "ACTIONS = 6 # number of valid actions\n",
    "GAMMA = 0.99 # decay rate of past observations\n",
    "OBSERVE = 1500. # timesteps to observe before training\n",
    "EXPLORE = 1000. # frames over which to anneal epsilon\n",
    "FINAL_EPSILON = 0.05 # final value of epsilon\n",
    "INITIAL_EPSILON = 1.0 # starting value of epsilon\n",
    "REPLAY_MEMORY = 590000 # number of previous transitions to remember\n",
    "BATCH = 32 # size of minibatch\n",
    "K = 1 # only select an action every Kth frame, repeat prev for others\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.01)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.01, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, stride):\n",
    "    return tf.nn.conv2d(x, W, strides = [1, stride, stride, 1], padding = \"SAME\")\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \"SAME\")\n",
    "\n",
    "def createNetwork():\n",
    "    # network weights\n",
    "    W_conv1 = weight_variable([8, 8, 4, 32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "\n",
    "    W_conv2 = weight_variable([4, 4, 32, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "\n",
    "    W_conv3 = weight_variable([3, 3, 64, 64])\n",
    "    b_conv3 = bias_variable([64])\n",
    "    \n",
    "    W_fc1 = weight_variable([1600, 512])\n",
    "    b_fc1 = bias_variable([512])\n",
    "\n",
    "    W_fc2 = weight_variable([512, ACTIONS])\n",
    "    b_fc2 = bias_variable([ACTIONS])\n",
    "\n",
    "    # input layer\n",
    "    s = tf.placeholder(\"float\", [None, 80, 80, 4])\n",
    "\n",
    "    # hidden layers\n",
    "    h_conv1 = tf.nn.sigmoid(conv2d(s, W_conv1, 4) + b_conv1)\n",
    "    h_conv1=h_conv1*(conv2d(s, W_conv1, 4) + b_conv1)\n",
    "    \n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "    h_conv2 = tf.nn.sigmoid(conv2d(h_pool1, W_conv2, 2) + b_conv2)\n",
    "    h_conv2 = h_conv2*(conv2d(h_pool1, W_conv2, 2) + b_conv2)\n",
    "    #h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "    h_conv3 = tf.nn.sigmoid(conv2d(h_conv2, W_conv3, 1) + b_conv3)\n",
    "    h_conv3 = h_conv3*(conv2d(h_conv2, W_conv3, 1) + b_conv3)\n",
    "    #h_pool3 = max_pool_2x2(h_conv3)\n",
    "\n",
    "    #h_pool3_flat = tf.reshape(h_pool3, [-1, 256])\n",
    "    h_conv3_flat = tf.reshape(h_conv3, [-1, 1600])\n",
    "\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_conv3_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    # readout layer\n",
    "    readout = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "    \n",
    "    tf.Summary\n",
    "\n",
    "    return s, readout, h_fc1\n",
    "\n",
    "def trainNetwork(s, readout, h_fc1, sess):\n",
    "    # define the cost function\n",
    "    a = tf.placeholder(\"float\", [None, ACTIONS])\n",
    "    y = tf.placeholder(\"float\", [None])\n",
    "    readout_action = tf.reduce_sum(tf.multiply(readout, a), reduction_indices = 1)\n",
    "    cost = tf.reduce_mean(tf.square(y - readout_action))\n",
    "    train_step = tf.train.AdamOptimizer(1e-6).minimize(cost)\n",
    "\n",
    "    # open up a game state to communicate with emulator\n",
    "    game_state = game.GameState()\n",
    "#########################################Observing####################################################\n",
    "    # store the previous observations in replay memory\n",
    "    D = deque()\n",
    "\n",
    "    # printing\n",
    "    a_file = open(\"logs_\" + GAME + \"/readout.txt\", 'w')\n",
    "    h_file = open(\"logs_\" + GAME + \"/hidden.txt\", 'w')\n",
    "\n",
    "    # get the first state by doing nothing and preprocess the image to 80x80x4\n",
    "    do_nothing = np.zeros(ACTIONS)\n",
    "    do_nothing[0] = 1\n",
    "    x_t, r_0, terminal = game_state.frame_step(do_nothing)\n",
    "    x_t = cv2.cvtColor(cv2.resize(x_t, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
    "    ret, x_t = cv2.threshold(x_t,1,255,cv2.THRESH_BINARY)\n",
    "    s_t = np.stack((x_t, x_t, x_t, x_t), axis = 2)\n",
    "\n",
    "    # saving and loading networks\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    checkpoint = tf.train.get_checkpoint_state(\"saved_networks\")\n",
    "    if checkpoint and checkpoint.model_checkpoint_path:\n",
    "        saver.restore(sess, checkpoint.model_checkpoint_path)\n",
    "        print \"Successfully loaded:\", checkpoint.model_checkpoint_path\n",
    "    else:\n",
    "        print \"Could not find old network weights\"\n",
    "\n",
    "    epsilon = INITIAL_EPSILON\n",
    "    t = 0\n",
    "    while \"pigs\" != \"fly\":\n",
    "        # choose an action epsilon greedily\n",
    "        readout_t = readout.eval(feed_dict = {s : [s_t]})[0]\n",
    "        a_t = np.zeros([ACTIONS])\n",
    "        action_index = 0\n",
    "        if random.random() <= epsilon or t <= OBSERVE:\n",
    "            action_index = random.randrange(ACTIONS)\n",
    "            a_t[action_index] = 1\n",
    "        else:\n",
    "            action_index = np.argmax(readout_t)\n",
    "            a_t[action_index] = 1\n",
    "\n",
    "        # scale down epsilon\n",
    "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
    "\n",
    "        for i in range(0, K):\n",
    "            # run the selected action and observe next state and reward\n",
    "            x_t1_col, r_t, terminal = game_state.frame_step(a_t)\n",
    "            x_t1 = cv2.cvtColor(cv2.resize(x_t1_col, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
    "            ret, x_t1 = cv2.threshold(x_t1,1,255,cv2.THRESH_BINARY)\n",
    "            x_t1 = np.reshape(x_t1, (80, 80, 1))\n",
    "            s_t1 = np.append(x_t1, s_t[:,:,0:3], axis = 2)\n",
    "\n",
    "            # store the transition in D\n",
    "            D.append((s_t, a_t, r_t, s_t1, terminal))\n",
    "            if len(D) > REPLAY_MEMORY:\n",
    "                D.popleft()\n",
    "\n",
    "        # only train if done observing\n",
    "        if t > OBSERVE:\n",
    "            print(\"finish observe\")\n",
    "            # sample a minibatch to train on\n",
    "            minibatch = random.sample(D, BATCH)\n",
    "\n",
    "            # get the batch variables\n",
    "            s_j_batch = [d[0] for d in minibatch]\n",
    "            a_batch = [d[1] for d in minibatch]\n",
    "            r_batch = [d[2] for d in minibatch]\n",
    "            s_j1_batch = [d[3] for d in minibatch]\n",
    "\n",
    "            y_batch = []\n",
    "            readout_j1_batch = readout.eval(feed_dict = {s : s_j1_batch})\n",
    "            for i in range(0, len(minibatch)):\n",
    "                # if terminal only equals reward\n",
    "                if minibatch[i][4]:\n",
    "                    y_batch.append(r_batch[i])\n",
    "                else:\n",
    "                    y_batch.append(r_batch[i] + GAMMA * np.max(readout_j1_batch[i]))\n",
    "\n",
    "            # perform gradient step\n",
    "            train_step.run(feed_dict = {\n",
    "                y : y_batch,\n",
    "                a : a_batch,\n",
    "                s : s_j_batch})\n",
    "\n",
    "        # update the old values\n",
    "        s_t = s_t1\n",
    "        t += 1\n",
    "\n",
    "        # save progress every 10000 iterations\n",
    "        #if t % 10000 == 0:\n",
    "         \n",
    "#   saver.save(sess, 'saved_networks/' + GAME + '-dqn', global_step = t)\n",
    "\n",
    "        # print info\n",
    "        state = \"\"\n",
    "        if t <= OBSERVE:\n",
    "            state = \"observe\"\n",
    "        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
    "            state = \"explore\"\n",
    "        else:\n",
    "            state = \"train\"\n",
    "        print \"TIMESTEP\", t, \"/ STATE\", state, \"/ LINES\", game_state.total_lines, \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t, \"/ Q_MAX %e\" % np.max(readout_t)\n",
    "\n",
    "        # write info to files\n",
    "        '''\n",
    "        if t % 10000 <= 100:\n",
    "            a_file.write(\",\".join([str(x) for x in readout_t]) + '\\n')\n",
    "            h_file.write(\",\".join([str(x) for x in h_fc1.eval(feed_dict={s:[s_t]})[0]]) + '\\n')\n",
    "            cv2.imwrite(\"logs_tetris/frame\" + str(t) + \".png\", x_t1)\n",
    "        '''\n",
    "\n",
    "def playGame():\n",
    "    sess = tf.InteractiveSession()\n",
    "    s, readout, h_fc1 = createNetwork()\n",
    "    trainNetwork(s, readout, h_fc1, sess)\n",
    "\n",
    "def main():\n",
    "    playGame()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python2]",
   "language": "python",
   "name": "conda-env-python2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
